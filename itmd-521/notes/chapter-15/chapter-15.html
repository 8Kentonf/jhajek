<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Chapter 15" />
  <title>Spark the Definitive Guide 2nd Edition</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Spark the Definitive Guide 2nd Edition</h1>
  <p class="author">
Chapter 15
  </p>
  <p class="date">How Spark Runs on a Cluster</p>
</div>
<div id="how-spark-runs-on-a-cluster" class="title-slide slide section level1"><h1>How Spark Runs on a Cluster</h1></div><div id="text-book" class="slide section level2">
<h1>Text Book</h1>
<div class="figure">
<img src="images/spark-book.png" title="Spark TextBook" alt="itmd-521 textbook" />
<p class="caption"><em>itmd-521 textbook</em></p>
</div>
</div><div id="objectives-and-outcomes" class="slide section level2">
<h1>Objectives and Outcomes</h1>
<ul class="incremental">
<li>Introduce and discuss what happens when Spark executes code</li>
<li>Understand the architecture and components of a Spark Application</li>
<li>Understand and discuss Spark pipelining</li>
<li>Understand the requirements to run a Spark Application (leaning into chapter 16)</li>
</ul>
</div><div id="review---214" class="slide section level2">
<h1>Review - 214</h1>
<ul class="incremental">
<li>Thus far we have…
<ul class="incremental">
<li>Focused on Spark’s properties as a programming interface</li>
<li>Discussed how the structured APIs take a logical operation</li>
<li>Break it into a logical plan</li>
<li>Convert that to a physical plan that consists of RDD operations</li>
<li>Executes them across a cluster of machines</li>
</ul></li>
<li>Learned the 6 core filetypes and their pros and cons</li>
</ul>
</div><div id="the-architecture-of-a-spark-application" class="slide section level2">
<h1>The Architecture of a Spark Application</h1>
<ul class="incremental">
<li>Some review from the mid-term:</li>
<li>The Spark driver
<ul class="incremental">
<li>The controller of the execution of a Spark Application</li>
<li>Maintains all of the state of the Spark cluster</li>
<li>It must interface with the cluster manager to get physical resources</li>
<li>Ultimately a process for maintaining the state of the application running on the cluster</li>
</ul></li>
<li>Spark Executors
<ul class="incremental">
<li>These are processes that perform the tasks assigned by the Spark driver</li>
<li>Report their state back (success or failure)</li>
<li>There can be multiple <strong>executor</strong> processes</li>
</ul></li>
</ul>
</div><div id="the-architecture-of-a-spark-application---new-parts" class="slide section level2">
<h1>The Architecture of a Spark Application - New Parts</h1>
<ul class="incremental">
<li>The Cluster Manager
<ul class="incremental">
<li>The driver and executors do not exist in a vacuum</li>
<li>A cluster manager maintains a cluster of machines that will run your Spark Application</li>
<li>A cluster has its own “driver” process and “worker” processes (separate from Spark)</li>
<li>These processes are tied to physical machines (cluster nodes)</li>
</ul></li>
<li>When we run a Spark Application we request resources from the cluster manager to run it
<ul class="incremental">
<li>The cluster manager is responsible for running the underlying computers</li>
<li><img src="images/figure-15-1.png" title="fig:Figure 15-1 A Cluster Driver" alt="Figure 15-1. A Cluster Driver" /></li>
</ul></li>
</ul>
</div><div id="execution-modes" class="slide section level2">
<h1>Execution Modes</h1>
<ul class="incremental">
<li>There are <a href="https://spark.apache.org/docs/latest/" title="Spark documentation on cluster types">four types of cluster manager types</a> available (Spark 2.4.5)
<ul class="incremental">
<li>standalone cluster manager</li>
<li>Apache Mesos</li>
<li><a href="https://spark.apache.org/docs/latest/running-on-yarn.html" title="Apache YARN documentation">Hadoop YARN</a></li>
<li>Kubernetes</li>
</ul></li>
<li>We will be focusing on Hadoop YARN as that is what we have setup and running already<br />
</li>
<li>There are three execution modes
<ul class="incremental">
<li>This is how you tell the Spark application how to distribute your application</li>
<li>This gives you the power to determine where the resources are located when you run the application
<ul class="incremental">
<li>Cluster mode</li>
<li>Client mode</li>
<li>Local mode</li>
</ul></li>
</ul></li>
<li>We will be moving away from the shell (pyspark or spark-shell) and to compiled code for submission to a cluster</li>
</ul>
</div><div id="cluster-mode" class="slide section level2">
<h1>Cluster Mode</h1>
<ul class="incremental">
<li>Cluster is the most common way of running Spark Applications
<ul class="incremental">
<li>User submits a precompiled JAR file, Python, or R script</li>
<li>Runs completely on the cluster</li>
<li>Cluster manager then launches the <strong>driver</strong> process on a worker node
<ul class="incremental">
<li>In addition to the <strong>executor</strong> processes</li>
<li>This means that the cluster manager is responsible for maintaining all Spark Application processes</li>
</ul></li>
<li><img src="images/figure-15-2.png" title="fig:Spark&#39;s cluster mode" alt="Figure 15-2. Spark’s Cluster Mode" /></li>
</ul></li>
</ul>
</div><div id="client-mode" class="slide section level2">
<h1>Client Mode</h1>
<ul class="incremental">
<li>Almost the same as <strong>cluster mode</strong> except that the spark driver remains on the client machine that submitted the application
<ul class="incremental">
<li>This could be your laptop in this case</li>
<li>The disadvantage is that you can’t shutdown your laptop!</li>
<li>Client machines are responsible for maintaining <strong>driver</strong> process</li>
<li>Cluster manager maintains the <strong>executor</strong> processes</li>
</ul></li>
<li>The driver is running on a machine outside of the cluster
<ul class="incremental">
<li>Workers are located on machines in the cluster</li>
<li><img src="images/figure-15-3.png" title="fig:Spark&#39;s Client Mode" alt="Figure 15-3 Spark’s Client Mode" /></li>
</ul></li>
</ul>
</div><div id="local-mode" class="slide section level2">
<h1>Local Mode</h1>
<ul class="incremental">
<li>Different from the above two modes
<ul class="incremental">
<li>Local mode means the entire cluster runs on a single machine</li>
<li>Good for quick testing</li>
<li>Good for our class as there is zero setup</li>
<li>Good way to learn Spark</li>
</ul></li>
</ul>
</div><div id="spark-application-life-cycle" class="slide section level2">
<h1>Spark Application Life Cycle</h1>
<ul class="incremental">
<li>Now, how does it work?
<ul class="incremental">
<li>Here is an illustrated example</li>
<li>4 node cluster
<ul class="incremental">
<li>1 driver node (cluster manager driver node, not Spark node)</li>
<li>The type is irrelevant for this example)</li>
<li>3 worker nodes</li>
</ul></li>
</ul></li>
<li>Client request
<ul class="incremental">
<li><img src="images/figure-15-4.png" title="Requesting resources for a driver" alt="Figure 15-4. Requesting resources for a driver" /></li>
<li>We take a precompiled JAR and it is submitted to the Spark driver nodes</li>
<li>To do this we would run code like this: <code>spark-submit --class &lt;main-class&gt; --master &lt;master-url&gt;  --deploy-mode cluster &lt;application-jar name&gt;</code></li>
</ul></li>
</ul>
</div><div id="launch" class="slide section level2">
<h1>Launch</h1>
<ul class="incremental">
<li>Once the driver process has been placed on the cluster, it begins running user code
<ul class="incremental">
<li>The code contains a <code>SparkSession</code> object</li>
<li>The <code>SparkSession</code> will communicate with the cluster manager (darker line)</li>
<li>Asking to launch Spark executor processes across the cluster (lighter lines)</li>
<li>Number of executors and partitions are defined on the command line dynamically or in the application code</li>
<li><img src="images/figure-15-5.png" title="fig:Launch the Spark Application" alt="Figure 15-5. Launching the Spark Application" /></li>
</ul></li>
</ul>
</div><div id="execution-and-completion" class="slide section level2">
<h1>Execution and Completion</h1>
<ul class="incremental">
<li>The drivers and the workers communicate among themselves, moving code around
<ul class="incremental">
<li><img src="images/figure-15-6.png" title="Application Execution" alt="Figure 15-6. Application Execution" /></li>
<li><img src="images/figure-15-7.png" title="fig:Shutting down the application" alt="Figure 15-7. Shutting down the application" /></li>
</ul></li>
</ul>
</div><div id="sample" class="slide section level2">
<h1>Sample</h1>
<ul class="incremental">
<li>sample 1</li>
</ul>
</div><div id="conclusion" class="slide section level2">
<h1>Conclusion</h1>
<ul class="incremental">
<li>We walked through the six core data-sources.</li>
<li>We walked through the various read and write options for these six datatypes</li>
<li>We covered reading and writing data in parallel</li>
<li>We explained partitioning and bucketing in relation to writing data</li>
</ul>
</div><div id="questions" class="slide section level2">
<h1>Questions</h1>
<ul class="incremental">
<li>Any questions?</li>
<li>For next time, read Chapter 15 &amp; 16 and do any exercises in the book.</li>
</ul>
</div>
</body>
</html>
